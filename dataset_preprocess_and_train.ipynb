{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57de3e7b",
   "metadata": {},
   "source": [
    "# Skin Disease Detection using Mobile Application\n",
    "## Final Year Project 2\n",
    "Ahmad Daniel Ikhwan Bin Rosli <br>\n",
    "1201103071"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd74c1",
   "metadata": {},
   "source": [
    "### Dataset Preprocessing and Model Trainning \n",
    "\n",
    "In this notebook we wil preprocess our final_dataset. Then we will use the preprocessed final_dataset to train and compare three lightweight deep learing architectures:\n",
    "- MobileNet\n",
    "- MobileNetV2\n",
    "- MobileNetV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a2ca911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import MobileNet, MobileNetV2, MobileNetV3Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21e5899",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170ce35c",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22c098e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset_path = Path(\"datasets/final_dataset\")\n",
    "img_size = (224, 224)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f6cec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6094 files belonging to 10 classes.\n",
      "Found 758 files belonging to 10 classes.\n",
      "Found 770 files belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# load final_datasets\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    final_dataset_path / \"train\", label_mode=\"categorical\", batch_size=batch_size, shuffle=True, seed=42\n",
    ")\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    final_dataset_path / \"val\", label_mode=\"categorical\", batch_size=batch_size, shuffle=False\n",
    ")\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    final_dataset_path / \"test\", label_mode=\"categorical\", batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abb4a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize with padding and normalize it\n",
    "def resize_with_padding(image, label):\n",
    "    image = tf.image.resize_with_pad(image, 224, 224)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(resize_with_padding).cache().prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.map(resize_with_padding).cache().prefetch(AUTOTUNE)\n",
    "test_ds = test_ds.map(resize_with_padding).cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b28a67e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image augmentation we do this before training to make it as a controled variable so we can compare each model fairly later on\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),                      \n",
    "    layers.RandomRotation(0.1),                           \n",
    "    layers.RandomZoom(0.1),                               \n",
    "    layers.RandomTranslation(0.1, 0.1),                   \n",
    "    layers.RandomBrightness(factor=0.2)                   \n",
    "])\n",
    "\n",
    "def preprocess_with_augmentation(image, label):\n",
    "    image = data_augmentation(image, training=True)\n",
    "    image = tf.image.resize_with_pad(image, 224, 224)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(preprocess_with_augmentation).cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab6f607",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc20877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model builder first layer\n",
    "def build_model(base_model, input_shape=(224, 224, 3), num_classes=10):\n",
    "    base_model.trainable = False\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        base_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "201b1dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# our models that will be trained\n",
    "base_models = {\n",
    "    \"MobileNet\": MobileNet(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3)),\n",
    "    \"MobileNetV2\": MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3)),\n",
    "    \"MobileNetV3Small\": MobileNetV3Small(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72df79f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: MobileNet\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 506ms/step - accuracy: 0.1327 - loss: 2.2963 - val_accuracy: 0.0897 - val_loss: 2.8984\n",
      "Epoch 2/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 369ms/step - accuracy: 0.1366 - loss: 2.2838 - val_accuracy: 0.0937 - val_loss: 3.0536\n",
      "Epoch 3/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 381ms/step - accuracy: 0.1390 - loss: 2.2680 - val_accuracy: 0.1108 - val_loss: 3.3925\n",
      "Epoch 4/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 365ms/step - accuracy: 0.1384 - loss: 2.2733 - val_accuracy: 0.1121 - val_loss: 3.7982\n",
      "Epoch 5/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 358ms/step - accuracy: 0.1411 - loss: 2.2682 - val_accuracy: 0.1108 - val_loss: 4.3211\n",
      "Epoch 6/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 357ms/step - accuracy: 0.1469 - loss: 2.2630 - val_accuracy: 0.1069 - val_loss: 4.9955\n",
      "Epoch 7/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 348ms/step - accuracy: 0.1400 - loss: 2.2640 - val_accuracy: 0.1055 - val_loss: 5.5518\n",
      "Epoch 8/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 352ms/step - accuracy: 0.1449 - loss: 2.2619 - val_accuracy: 0.1042 - val_loss: 6.2050\n",
      "Epoch 9/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 356ms/step - accuracy: 0.1390 - loss: 2.2589 - val_accuracy: 0.1042 - val_loss: 6.7449\n",
      "Epoch 10/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 359ms/step - accuracy: 0.1383 - loss: 2.2629 - val_accuracy: 0.1042 - val_loss: 7.4275\n",
      "Training model: MobileNetV2\n",
      "Epoch 1/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 421ms/step - accuracy: 0.1294 - loss: 2.4275 - val_accuracy: 0.0712 - val_loss: 2.6670\n",
      "Epoch 2/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 433ms/step - accuracy: 0.1322 - loss: 2.3736 - val_accuracy: 0.0712 - val_loss: 2.6946\n",
      "Epoch 3/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 425ms/step - accuracy: 0.1351 - loss: 2.3472 - val_accuracy: 0.0884 - val_loss: 2.6955\n",
      "Epoch 4/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 421ms/step - accuracy: 0.1331 - loss: 2.3469 - val_accuracy: 0.1055 - val_loss: 2.7899\n",
      "Epoch 5/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 428ms/step - accuracy: 0.1297 - loss: 2.3356 - val_accuracy: 0.1095 - val_loss: 2.8884\n",
      "Epoch 6/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 427ms/step - accuracy: 0.1455 - loss: 2.3242 - val_accuracy: 0.1108 - val_loss: 2.9680\n",
      "Epoch 7/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 452ms/step - accuracy: 0.1376 - loss: 2.3154 - val_accuracy: 0.1082 - val_loss: 3.0469\n",
      "Epoch 8/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 429ms/step - accuracy: 0.1326 - loss: 2.3155 - val_accuracy: 0.1055 - val_loss: 3.2198\n",
      "Epoch 9/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 452ms/step - accuracy: 0.1433 - loss: 2.3073 - val_accuracy: 0.1069 - val_loss: 3.3539\n",
      "Epoch 10/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 434ms/step - accuracy: 0.1330 - loss: 2.3140 - val_accuracy: 0.0989 - val_loss: 3.4473\n",
      "Training model: MobileNetV3Small\n",
      "Epoch 1/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 128ms/step - accuracy: 0.1341 - loss: 2.2925 - val_accuracy: 0.1464 - val_loss: 2.2547\n",
      "Epoch 2/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 123ms/step - accuracy: 0.1334 - loss: 2.2790 - val_accuracy: 0.1491 - val_loss: 2.2539\n",
      "Epoch 3/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 127ms/step - accuracy: 0.1355 - loss: 2.2727 - val_accuracy: 0.1478 - val_loss: 2.2547\n",
      "Epoch 4/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 126ms/step - accuracy: 0.1331 - loss: 2.2721 - val_accuracy: 0.0937 - val_loss: 2.2539\n",
      "Epoch 5/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 125ms/step - accuracy: 0.1336 - loss: 2.2737 - val_accuracy: 0.1135 - val_loss: 2.2544\n",
      "Epoch 6/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 125ms/step - accuracy: 0.1345 - loss: 2.2682 - val_accuracy: 0.1280 - val_loss: 2.2551\n",
      "Epoch 7/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 126ms/step - accuracy: 0.1342 - loss: 2.2673 - val_accuracy: 0.1135 - val_loss: 2.2546\n",
      "Epoch 8/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 130ms/step - accuracy: 0.1365 - loss: 2.2682 - val_accuracy: 0.1108 - val_loss: 2.2544\n",
      "Epoch 9/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 134ms/step - accuracy: 0.1304 - loss: 2.2671 - val_accuracy: 0.1438 - val_loss: 2.2547\n",
      "Epoch 10/10\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 131ms/step - accuracy: 0.1343 - loss: 2.2642 - val_accuracy: 0.1438 - val_loss: 2.2547\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "histories = {}\n",
    "\n",
    "for name, base_model in base_models.items():\n",
    "    print(f\"Training model: {name}\")\n",
    "    \n",
    "    model = build_model(base_model)  \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=10,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    histories[name] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c414f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
